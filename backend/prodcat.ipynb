{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepackaged_job_solutions = [\n",
    "    \"Account Manager Solution\",\n",
    "    \"Administrative Professional - Short Form\",\n",
    "    \"Agency Manager Solution\",\n",
    "    \"Apprentice + 8.0 Job Focused Assessment\",\n",
    "    \"Apprentice 8.0 Job Focused Assessment\",\n",
    "    \"Bank Administrative Assistant - Short Form\",\n",
    "    \"Bank Collections Agent - Short Form\",\n",
    "    \"Bank Operations Supervisor - Short Form\",\n",
    "    \"Bilingual Spanish Reservation Agent Solution\",\n",
    "    \"Bookkeeping, Accounting, Auditing Clerk - Short Form\",\n",
    "    \"Branch Manager - Short Form\",\n",
    "    \"Cashier Solution\",\n",
    "    \"Customer Service Representative Solution\",\n",
    "    \"Data Entry Operator Solution\",\n",
    "    \"Financial Analyst Solution\",\n",
    "    \"Graduate Hiring Solution\",\n",
    "    \"Insurance Sales Agent Solution\",\n",
    "    \"IT Professional Solution\",\n",
    "    \"Manager Hiring Solution\",\n",
    "    \"Manufacturing Hiring Solution\",\n",
    "    \"Prep/Line Cook Solution\",\n",
    "    \"Professional Hiring Solution\",\n",
    "    \"Retail Hiring Solution\",\n",
    "    \"Sales Hiring Solution\",\n",
    "    \"Sales Professional Solution\",\n",
    "    \"Software Developer Solution\",\n",
    "    \"Technology Hiring Solution\",\n",
    "    \"Call Center Representative Solution\",\n",
    "    \"Customer Support Specialist Solution\",\n",
    "    \"Field Sales Representative Solution\",\n",
    "    \"Marketing Coordinator Solution\",\n",
    "    \"Network Administrator Solution\",\n",
    "    \"Project Manager Solution\",\n",
    "    \"Quality Assurance Analyst Solution\",\n",
    "    \"Retail Store Manager Solution\",\n",
    "    \"Technical Support Engineer Solution\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepackaged_job_solutions = [\n",
    "    \"Account-Manager-Solution\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Scraping: https://www.shl.com/solutions/products/product-catalog/view/account-manager-solution/\n",
      "Launching chrome browser...\n",
      "‚ùå Error scraping account-manager-solution: '\\n    \"assessment_name\"'\n",
      "üîç Scraping: https://www.shl.com/solutions/products/product-catalog/view/data-analyst-solution/\n",
      "Launching chrome browser...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "from scrape import scrape_website, extract_body_content, clean_body_content, split_dom_content\n",
    "from google_parse import parse_with_gemini  # Ensure it uses the new definition below\n",
    "\n",
    "# Your list of job slugs (example)\n",
    "job_slugs = [\"account-manager-solution\"]\n",
    "\n",
    "# Output file\n",
    "output_path = \"job_descriptions.json\"\n",
    "\n",
    "# Load existing data if available\n",
    "if os.path.exists(output_path):\n",
    "    with open(output_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        existing_data = json.load(f)\n",
    "else:\n",
    "    existing_data = []\n",
    "\n",
    "# Helper set to track processed slugs\n",
    "existing_slugs = {entry[\"slug\"] for entry in existing_data}\n",
    "\n",
    "# Loop through all job slugs\n",
    "for slug in job_slugs:\n",
    "    if slug in existing_slugs:\n",
    "        print(f\"‚úÖ Skipping {slug} (already processed)\")\n",
    "        continue\n",
    "\n",
    "    url = f\"https://www.shl.com/solutions/products/product-catalog/view/{slug}/\"\n",
    "    print(f\"üîç Scraping: {url}\")\n",
    "\n",
    "    try:\n",
    "        html = scrape_website(url)\n",
    "        body = extract_body_content(html)\n",
    "        cleaned = clean_body_content(body)\n",
    "\n",
    "        # LLM recommendation step\n",
    "        dom_chunks = split_dom_content(cleaned)\n",
    "        llm_output = parse_with_gemini(dom_chunks, slug)\n",
    "        print(f\"üîç LLM output: {llm_output}\")\n",
    "\n",
    "        if isinstance(llm_output, list):  # If it's a list of strings\n",
    "            llm_output = llm_output[0]\n",
    "\n",
    "        try:\n",
    "            parsed_recommendations = json.loads(llm_output)\n",
    "        except Exception as json_err:\n",
    "            print(f\"‚ö†Ô∏è JSON parsing failed for {slug}: {json_err}\")\n",
    "            parsed_recommendations = []\n",
    "\n",
    "        # Add entry to JSON data\n",
    "        job_entry = {\n",
    "            \"slug\": slug,\n",
    "            \"recommendations\": parsed_recommendations\n",
    "        }\n",
    "        existing_data.append(job_entry)\n",
    "\n",
    "        # Save JSON after every successful job\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(existing_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        print(f\"‚úÖ Processed and saved: {slug}\")\n",
    "        time.sleep(2)  # Avoid hitting rate limits\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error scraping {slug}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Models:\n",
      "- Name: models/chat-bison-001\n",
      "  Description: A legacy text-only model optimized for chat conversations\n",
      "  Supported Generation Methods: ['generateMessage', 'countMessageTokens']\n",
      "  Input Token Limit: 4096\n",
      "  Output Token Limit: 1024\n",
      "--------------------\n",
      "- Name: models/text-bison-001\n",
      "  Description: A legacy model that understands text and generates text as an output\n",
      "  Supported Generation Methods: ['generateText', 'countTextTokens', 'createTunedTextModel']\n",
      "  Input Token Limit: 8196\n",
      "  Output Token Limit: 1024\n",
      "--------------------\n",
      "- Name: models/embedding-gecko-001\n",
      "  Description: Obtain a distributed representation of a text.\n",
      "  Supported Generation Methods: ['embedText', 'countTextTokens']\n",
      "  Input Token Limit: 1024\n",
      "  Output Token Limit: 1\n",
      "--------------------\n",
      "- Name: models/gemini-1.0-pro-vision-latest\n",
      "  Description: The original Gemini 1.0 Pro Vision model version which was optimized for image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. Move to a newer Gemini version.\n",
      "  Supported Generation Methods: ['generateContent', 'countTokens']\n",
      "  Input Token Limit: 12288\n",
      "  Output Token Limit: 4096\n",
      "--------------------\n",
      "- Name: models/gemini-pro-vision\n",
      "  Description: The original Gemini 1.0 Pro Vision model version which was optimized for image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. Move to a newer Gemini version.\n",
      "  Supported Generation Methods: ['generateContent', 'countTokens']\n",
      "  Input Token Limit: 12288\n",
      "  Output Token Limit: 4096\n",
      "--------------------\n",
      "- Name: models/gemini-1.5-pro-latest\n",
      "  Description: Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens.\n",
      "  Supported Generation Methods: ['generateContent', 'countTokens']\n",
      "  Input Token Limit: 2000000\n",
      "  Output Token Limit: 8192\n",
      "--------------------\n",
      "- Name: models/gemini-1.5-pro-001\n",
      "  Description: Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in May of 2024.\n",
      "  Supported Generation Methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "  Input Token Limit: 2000000\n",
      "  Output Token Limit: 8192\n",
      "--------------------\n",
      "- Name: models/gemini-1.5-pro-002\n",
      "  Description: Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in September of 2024.\n",
      "  Supported Generation Methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "  Input Token Limit: 2000000\n",
      "  Output Token Limit: 8192\n",
      "--------------------\n",
      "- Name: models/gemini-1.5-pro\n",
      "  Description: Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in May of 2024.\n",
      "  Supported Generation Methods: ['generateContent', 'countTokens']\n",
      "  Input Token Limit: 2000000\n",
      "  Output Token Limit: 8192\n",
      "--------------------\n",
      "- Name: models/gemini-1.5-flash-latest\n",
      "  Description: Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks.\n",
      "  Supported Generation Methods: ['generateContent', 'countTokens']\n",
      "  Input Token Limit: 1000000\n",
      "  Output Token Limit: 8192\n",
      "--------------------\n",
      "- Name: models/gemini-1.5-flash-001\n",
      "  Description: Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in May of 2024.\n",
      "  Supported Generation Methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "  Input Token Limit: 1000000\n",
      "  Output Token Limit: 8192\n",
      "--------------------\n",
      "- Name: models/gemini-1.5-flash-001-tuning\n",
      "  Description: Version of Gemini 1.5 Flash that supports tuning, our fast and versatile multimodal model for scaling across diverse tasks, released in May of 2024.\n",
      "  Supported Generation Methods: ['generateContent', 'countTokens', 'createTunedModel']\n",
      "  Input Token Limit: 16384\n",
      "  Output Token Limit: 8192\n",
      "--------------------\n",
      "- Name: models/gemini-1.5-flash\n",
      "  Description: Alias that points to the most recent stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks.\n",
      "  Supported Generation Methods: ['generateContent', 'countTokens']\n",
      "  Input Token Limit: 1000000\n",
      "  Output Token Limit: 8192\n",
      "--------------------\n",
      "- Name: models/gemini-1.5-flash-002\n",
      "  Description: Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in September of 2024.\n",
      "  Supported Generation Methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "  Input Token Limit: 1000000\n",
      "  Output Token Limit: 8192\n",
      "--------------------\n",
      "- Name: models/gemini-1.5-flash-8b\n",
      "  Description: Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.\n",
      "  Supported Generation Methods: ['createCachedContent', 'generateContent', 'countTokens']\n",
      "  Input Token Limit: 1000000\n",
      "  Output Token Limit: 8192\n",
      "--------------------\n",
      "- Name: models/gemini-1.5-flash-8b-001\n",
      "  Description: Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.\n",
      "  Supported Generation Methods: ['createCachedContent', 'generateContent', 'countTokens']\n",
      "  Input Token Limit: 1000000\n",
      "  Output Token Limit: 8192\n",
      "--------------------\n",
      "- Name: models/gemini-1.5-flash-8b-latest\n",
      "  Description: Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.\n",
      "  Supported Generation Methods: ['createCachedContent', 'generateContent', 'countTokens']\n",
      "  Input Token Limit: 1000000\n",
      "  Output Token Limit: 8192\n",
      "--------------------\n",
      "- Name: models/gemini-1.5-flash-8b-exp-0827\n",
      "  Description: Experimental release (August 27th, 2024) of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model. Replaced by Gemini-1.5-flash-8b-001 (stable).\n",
      "  Supported Generation Methods: ['generateContent', 'countTokens']\n",
      "  Input Token Limit: 1000000\n",
      "  Output Token Limit: 8192\n",
      "--------------------\n",
      "- Name: models/gemini-1.5-flash-8b-exp-0924\n",
      "  Description: Experimental release (September 24th, 2024) of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model. Replaced by Gemini-1.5-flash-8b-001 (stable).\n",
      "  Supported Generation Methods: ['generateContent', 'countTokens']\n",
      "  Input Token Limit: 1000000\n",
      "  Output Token Limit: 8192\n",
      "--------------------\n",
      "- Name: models/gemini-2.5-pro-exp-03-25\n",
      "  Description: Experimental release (March 25th, 2025) of Gemini 2.5 Pro\n",
      "  Supported Generation Methods: ['generateContent', 'countTokens']\n",
      "  Input Token Limit: 1048576\n",
      "  Output Token Limit: 65536\n",
      "--------------------\n",
      "- Name: models/gemini-2.5-pro-preview-03-25\n",
      "  Description: Gemini 2.5 Pro Preview 03-25\n",
      "  Supported Generation Methods: ['generateContent', 'countTokens']\n",
      "  Input Token Limit: 1048576\n",
      "  Output Token Limit: 65536\n",
      "--------------------\n",
      "- Name: models/gemini-2.0-flash-exp\n",
      "  Description: Gemini 2.0 Flash Experimental\n",
      "  Supported Generation Methods: ['generateContent', 'countTokens', 'bidiGenerateContent']\n",
      "  Input Token Limit: 1048576\n",
      "  Output Token Limit: 8192\n",
      "--------------------\n",
      "- Name: models/gemini-2.0-flash\n",
      "  Description: Gemini 2.0 Flash\n",
      "  Supported Generation Methods: ['generateContent', 'countTokens']\n",
      "  Input Token Limit: 1048576\n",
      "  Output Token Limit: 8192\n",
      "--------------------\n",
      "- Name: models/gemini-2.0-flash-001\n",
      "  Description: Stable version of Gemini 2.0 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in January of 2025.\n",
      "  Supported Generation Methods: ['generateContent', 'countTokens']\n",
      "  Input Token Limit: 1048576\n",
      "  Output Token Limit: 8192\n",
      "--------------------\n",
      "- Name: models/gemini-2.0-flash-exp-image-generation\n",
      "  Description: Gemini 2.0 Flash (Image Generation) Experimental\n",
      "  Supported Generation Methods: ['generateContent', 'countTokens', 'bidiGenerateContent']\n",
      "  Input Token Limit: 1048576\n",
      "  Output Token Limit: 8192\n",
      "--------------------\n",
      "- Name: models/gemini-2.0-flash-lite-001\n",
      "  Description: Stable version of Gemini 2.0 Flash Lite\n",
      "  Supported Generation Methods: ['generateContent', 'countTokens']\n",
      "  Input Token Limit: 1048576\n",
      "  Output Token Limit: 8192\n",
      "--------------------\n",
      "- Name: models/gemini-2.0-flash-lite\n",
      "  Description: Gemini 2.0 Flash-Lite\n",
      "  Supported Generation Methods: ['generateContent', 'countTokens']\n",
      "  Input Token Limit: 1048576\n",
      "  Output Token Limit: 8192\n",
      "--------------------\n",
      "- Name: models/gemini-2.0-flash-lite-preview-02-05\n",
      "  Description: Preview release (February 5th, 2025) of Gemini 2.0 Flash Lite\n",
      "  Supported Generation Methods: ['generateContent', 'countTokens']\n",
      "  Input Token Limit: 1048576\n",
      "  Output Token Limit: 8192\n",
      "--------------------\n",
      "- Name: models/gemini-2.0-flash-lite-preview\n",
      "  Description: Preview release (February 5th, 2025) of Gemini 2.0 Flash Lite\n",
      "  Supported Generation Methods: ['generateContent', 'countTokens']\n",
      "  Input Token Limit: 1048576\n",
      "  Output Token Limit: 8192\n",
      "--------------------\n",
      "- Name: models/gemini-2.0-pro-exp\n",
      "  Description: Experimental release (March 25th, 2025) of Gemini 2.5 Pro\n",
      "  Supported Generation Methods: ['generateContent', 'countTokens']\n",
      "  Input Token Limit: 1048576\n",
      "  Output Token Limit: 65536\n",
      "--------------------\n",
      "- Name: models/gemini-2.0-pro-exp-02-05\n",
      "  Description: Experimental release (March 25th, 2025) of Gemini 2.5 Pro\n",
      "  Supported Generation Methods: ['generateContent', 'countTokens']\n",
      "  Input Token Limit: 1048576\n",
      "  Output Token Limit: 65536\n",
      "--------------------\n",
      "- Name: models/gemini-exp-1206\n",
      "  Description: Experimental release (March 25th, 2025) of Gemini 2.5 Pro\n",
      "  Supported Generation Methods: ['generateContent', 'countTokens']\n",
      "  Input Token Limit: 1048576\n",
      "  Output Token Limit: 65536\n",
      "--------------------\n",
      "- Name: models/gemini-2.0-flash-thinking-exp-01-21\n",
      "  Description: Experimental release (January 21st, 2025) of Gemini 2.0 Flash Thinking\n",
      "  Supported Generation Methods: ['generateContent', 'countTokens']\n",
      "  Input Token Limit: 1048576\n",
      "  Output Token Limit: 65536\n",
      "--------------------\n",
      "- Name: models/gemini-2.0-flash-thinking-exp\n",
      "  Description: Experimental release (January 21st, 2025) of Gemini 2.0 Flash Thinking\n",
      "  Supported Generation Methods: ['generateContent', 'countTokens']\n",
      "  Input Token Limit: 1048576\n",
      "  Output Token Limit: 65536\n",
      "--------------------\n",
      "- Name: models/gemini-2.0-flash-thinking-exp-1219\n",
      "  Description: Gemini 2.0 Flash Thinking Experimental\n",
      "  Supported Generation Methods: ['generateContent', 'countTokens']\n",
      "  Input Token Limit: 1048576\n",
      "  Output Token Limit: 65536\n",
      "--------------------\n",
      "- Name: models/learnlm-1.5-pro-experimental\n",
      "  Description: Alias that points to the most recent stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens.\n",
      "  Supported Generation Methods: ['generateContent', 'countTokens']\n",
      "  Input Token Limit: 32767\n",
      "  Output Token Limit: 8192\n",
      "--------------------\n",
      "- Name: models/gemma-3-1b-it\n",
      "  Description: \n",
      "  Supported Generation Methods: ['generateContent', 'countTokens']\n",
      "  Input Token Limit: 32768\n",
      "  Output Token Limit: 8192\n",
      "--------------------\n",
      "- Name: models/gemma-3-4b-it\n",
      "  Description: \n",
      "  Supported Generation Methods: ['generateContent', 'countTokens']\n",
      "  Input Token Limit: 32768\n",
      "  Output Token Limit: 8192\n",
      "--------------------\n",
      "- Name: models/gemma-3-12b-it\n",
      "  Description: \n",
      "  Supported Generation Methods: ['generateContent', 'countTokens']\n",
      "  Input Token Limit: 32768\n",
      "  Output Token Limit: 8192\n",
      "--------------------\n",
      "- Name: models/gemma-3-27b-it\n",
      "  Description: \n",
      "  Supported Generation Methods: ['generateContent', 'countTokens']\n",
      "  Input Token Limit: 131072\n",
      "  Output Token Limit: 8192\n",
      "--------------------\n",
      "- Name: models/embedding-001\n",
      "  Description: Obtain a distributed representation of a text.\n",
      "  Supported Generation Methods: ['embedContent']\n",
      "  Input Token Limit: 2048\n",
      "  Output Token Limit: 1\n",
      "--------------------\n",
      "- Name: models/text-embedding-004\n",
      "  Description: Obtain a distributed representation of a text.\n",
      "  Supported Generation Methods: ['embedContent']\n",
      "  Input Token Limit: 2048\n",
      "  Output Token Limit: 1\n",
      "--------------------\n",
      "- Name: models/gemini-embedding-exp-03-07\n",
      "  Description: Obtain a distributed representation of a text.\n",
      "  Supported Generation Methods: ['embedContent']\n",
      "  Input Token Limit: 8192\n",
      "  Output Token Limit: 1\n",
      "--------------------\n",
      "- Name: models/gemini-embedding-exp\n",
      "  Description: Obtain a distributed representation of a text.\n",
      "  Supported Generation Methods: ['embedContent']\n",
      "  Input Token Limit: 8192\n",
      "  Output Token Limit: 1\n",
      "--------------------\n",
      "- Name: models/aqa\n",
      "  Description: Model trained to return answers to questions that are grounded in provided sources, along with estimating answerable probability.\n",
      "  Supported Generation Methods: ['generateAnswer']\n",
      "  Input Token Limit: 7168\n",
      "  Output Token Limit: 1024\n",
      "--------------------\n",
      "- Name: models/imagen-3.0-generate-002\n",
      "  Description: Vertex served Imagen 3.0 002 model\n",
      "  Supported Generation Methods: ['predict']\n",
      "  Input Token Limit: 480\n",
      "  Output Token Limit: 8192\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# Configure your API key (replace with your actual key)\n",
    "genai.configure(api_key=\"AIzaSyBXRVLF_2puAesfxgBQp3QV31frR0CNIHI\")\n",
    "\n",
    "try:\n",
    "    available_models = genai.list_models()\n",
    "    print(\"Available Models:\")\n",
    "    for model_info in available_models:\n",
    "        print(f\"- Name: {model_info.name}\")\n",
    "        print(f\"  Description: {model_info.description}\")\n",
    "        print(f\"  Supported Generation Methods: {model_info.supported_generation_methods}\")\n",
    "        print(f\"  Input Token Limit: {model_info.input_token_limit}\")\n",
    "        print(f\"  Output Token Limit: {model_info.output_token_limit}\")\n",
    "        #print(f\"  Model Version: {model_info.model_version}\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error listing models: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "c4env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
